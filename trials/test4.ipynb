{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import unicodedata\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nazimaniltepe/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = []\n",
    "for path, dirs, files in os.walk(f\"./TrainingSet\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".json\"):\n",
    "            f = open(os.path.join(path, file), encoding=\"utf-8-sig\")\n",
    "            json_data.append(json.load(f))\n",
    "            json_data[-1][\"FileName\"] = file\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_file = open('./stopwords2.txt')\n",
    "stop_words = sw_file.read()\n",
    "stop_words = stop_words.split(\"\\n\")\n",
    "sw_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_obj in json_data:\n",
    "    data_obj[\"Özet\"] = re.sub(r\"[-\\w\\.]+@([-\\w]+\\.)+[-\\w]{2,4}\", \"\", data_obj[\"Özet\"])\n",
    "    data_obj[\"Özet\"] = re.sub(r\"https?:\\/\\/(?:www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b(?:[-a-zA-Z0-9()@:%_\\+.~#?&\\/=]*)\", \"\", data_obj[\"Özet\"])\n",
    "    data_obj[\"Özet\"] = re.sub(r\"<([a-z]+)(?![^>]*\\/>)[^>]*>\", \"\", data_obj[\"Özet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19024 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "words_repr = []\n",
    "for data_obj in json_data:\n",
    "    vec = tokenizer(data_obj[\"Özet\"])\n",
    "    words.append(vec)\n",
    "    words_repr.append((data_obj[\"FileName\"], len(vec), vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "words_repr = []\n",
    "for data_obj in json_data:\n",
    "    vec = []\n",
    "    for i in sent_tokenize(data_obj[\"Özet\"], language=\"turkish\"):\n",
    "        for j in word_tokenize(i, language=\"turkish\"):\n",
    "            vec.append(j.lower())\n",
    "    words.append(vec)\n",
    "    words_repr.append((data_obj[\"FileName\"], len(vec), vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(words[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = gensim.models.Word2Vec(words, min_count = 1, vector_size = 100, window = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.03946626e+00,  9.95541811e-01,  4.26392764e-01, -1.50332658e-03,\n",
       "        2.69846201e-01, -1.19535792e+00,  9.16952252e-01,  1.57877541e+00,\n",
       "       -5.30536592e-01, -6.03813589e-01, -6.53959870e-01, -4.50643957e-01,\n",
       "       -4.79648292e-01,  7.38062441e-01,  1.07448649e+00, -6.57557964e-01,\n",
       "        6.93104208e-01,  1.64789140e-01, -5.74939251e-01, -6.57999396e-01,\n",
       "       -4.83407974e-02, -7.19854310e-02,  8.23006749e-01, -2.95457333e-01,\n",
       "       -1.46126747e-01, -1.76016420e-01, -2.06016660e-01,  1.69496641e-01,\n",
       "       -1.01063228e+00, -1.14812590e-01, -2.33470827e-01,  7.16659904e-01,\n",
       "       -7.52529874e-03, -5.22627890e-01, -4.26430792e-01,  1.16897011e+00,\n",
       "       -1.39112338e-01, -6.16025686e-01, -4.89752084e-01, -8.35157931e-01,\n",
       "       -1.36250541e-01, -4.57478434e-01, -1.20815836e-01,  1.61586285e-01,\n",
       "        8.90136480e-01,  1.86616942e-01, -8.47792998e-02,  2.55777210e-01,\n",
       "        1.82813630e-02,  8.33013117e-01,  2.72913694e-01, -2.31195405e-01,\n",
       "        1.97726876e-01, -4.92637187e-01,  3.10792804e-01, -1.15976274e-01,\n",
       "        3.59817266e-01, -1.51272863e-01, -3.77201289e-01,  3.57142448e-01,\n",
       "       -9.94022638e-02,  1.86789081e-01,  6.18504345e-01,  1.25045612e-01,\n",
       "       -1.83965540e+00,  6.09131634e-01,  2.80756563e-01,  5.26985347e-01,\n",
       "       -7.88431525e-01,  9.66619015e-01, -1.36675298e-01,  8.24482646e-03,\n",
       "        7.48035133e-01,  1.54565051e-01,  6.46112621e-01, -4.53028262e-01,\n",
       "        6.35843337e-01, -6.49255812e-01, -4.35822099e-01, -2.91401774e-01,\n",
       "       -1.84034199e-01, -2.55070060e-01, -5.43241382e-01,  5.97306609e-01,\n",
       "       -2.01163501e-01, -1.18737185e+00,  4.87643152e-01,  7.25879371e-01,\n",
       "       -5.76727502e-02,  4.97459531e-01,  9.14473951e-01,  6.83838069e-01,\n",
       "        4.44670588e-01, -1.11035630e-01,  1.48642266e+00,  7.30427921e-01,\n",
       "        1.62186399e-01, -4.70661074e-01,  5.28866291e-01,  5.93142033e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv['anonim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [(data_obj[\"FileName\"], clean_list(data_obj[\"Özet\"].split(\" \"))) for data_obj in json_data]\n",
    "words = [(w[0], len(w[1]), w[1]) for w in words]\n",
    "w_flat = [(w, wvec[0]) for wvec in words for w in wvec[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [(data_obj[\"FileName\"], clean_list(data_obj[\"Anahtar Kelimeler\"], True)) for data_obj in json_data]\n",
    "keywords = [(kw[0], len(kw[1]), kw[1]) for kw in keywords]\n",
    "kw_flat = [(kw, kwvec[0]) for kwvec in keywords for kw in kwvec[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_chars_and_words(list_flat):\n",
    "    list_unique = []\n",
    "    list_dict = {}\n",
    "    list_chars = []\n",
    "    list_charcodes = []\n",
    "    for w in list_flat:\n",
    "        if w[0] not in list_unique:\n",
    "            list_unique.append(w[0])\n",
    "            list_dict[w[0]] = [w[1]]\n",
    "        else:\n",
    "            list_dict[w[0]].append(w[1])\n",
    "        for c in w[0]:\n",
    "            if c not in list_chars:\n",
    "                list_chars.append(c)\n",
    "                list_charcodes.append((ord(c), w[0]))\n",
    "    list_chars = [[list_chars[i], list_charcodes[i][0], list_charcodes[i][1]] for i, c in enumerate(list_chars)]\n",
    "    list_unique = [[list_unique[i], len(list_dict[list_unique[i]]), list_dict[list_unique[i]]] for i,_ in enumerate(list_unique)]\n",
    "    return (list_chars, list_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_chars, kw_unique = unique_chars_and_words(kw_flat)\n",
    "w_chars, w_unique = unique_chars_and_words(w_flat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
