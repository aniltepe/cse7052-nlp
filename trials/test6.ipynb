{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = []\n",
    "for path, dirs, files in os.walk(f\"./TrainingSet\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".json\"):\n",
    "            f = open(os.path.join(path, file), encoding=\"utf-8-sig\")\n",
    "            json_data.append(json.load(f))\n",
    "            json_data[-1][\"FileName\"] = file\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAR_CONVERSION = {9:32,10:32,33:32,35:32,38:32,40:32,41:32,42:32,43:32,47:45,58:32,59:32,60:-1,61:32,62:-1,63:32,64:32,91:-1,93:-1,94:39,95:45,96:39,123:-1,124:32,125:-1,160:32,167:32,170:-1,171:32,176:-1,180:39,182:-1,184:-1,187:32,223:-1,224:97,225:97,226:97,228:97,229:97,230:101,232:101,233:101,234:101,235:101,236:105,237:105,238:105,239:105,241:110,242:111,243:111,244:111,245:111,248:111,249:117,250:117,251:117,257:97,261:97,263:99,269:99,279:101,299:105,322:108,324:110,353:115,363:117,369:252,382:122,523:105,537:351,601:101,699:39,700:39,703:39,706:-1,714:39,727:-1,774:{103: 287},775:{105: 105},8201:32,8208:45,8211:45,8212:45,8216:39,8217:39,8220:-1,8221:-1,8232:32, }\n",
    "STRING_CONVERSION = {\"$\": \"dolar\",\"…\": \" \",\"ﬀ\": \"ff\",\"ﬁ\": \"fi\",\"ﬂ \": \"fl\",\"ﬂ\": \"fl\",\"ﬄ\": \"ffl\",\"n°\": \"no\",\"N°\": \"no\",\"Anahtar Kelimeler:\": \"\",\"I\": \"ı\",\"İ\": \"i\",\"⅔\": \"%67\",\"⅕\": \"%20\",\"⅘\": \"%80\",\"⅚\": \"%83\",\"¼\": \"%25\",\"½\": \"%50\",\"¾\": \"%75\",}\n",
    "REGEX_CONVERSION = {r\"cov[ı,i]d-*\\s*19\": \"covid19\",r\"(\\smd?\\.?\\s{0,2})((\\d)+)\": r\" madde \\2\",r\"<([a-z]+)(?![^>]*\\/>)[^>]*>\": \"\",r\"\\.\": \" \",r\"-\": \" \",r\"'\": \" \",r\"(\\s(%)(\\s){0,2})((\\d)*)\": r\" yüzde \\4\", r\"%\": \"\", r\"hükumet\": \"hükümet\", r\"ışletme\": \"işletme\", r\"prepayment\": \"önödeme\", r\"arabulucui\": \"arabulucu\"}\n",
    "ABBREVIATIONS = {r\"(^|\\s)(smk)($|\\s|\\.)\":r\"\\1sınai mülkiyet kanunu\\3\", r\"(^|\\s)(t?mk)($|\\s|\\.)\": r\"\\1türk medeni kanunu\\3\", r\"(^|\\s)(t?ck)($|\\s|\\.)\": r\"\\1türk ceza kanunu\\3\", r\"(^|\\s)(t?bk)($|\\s|\\.)\": r\"\\1türk borçlar kanunu\\3\", r\"(^|\\s)(t?tk)($|\\s|\\.)\": r\"\\1türk ticaret kanunu\\3\", r\"(^|\\s)(t?vk)($|\\s|\\.)\": r\"\\1türk vergi kanunu\\3\", r\"(^|\\s)(a(b|t)ad)($|\\s|\\.)\": r\"\\1avrupa birliği adalet divanı\\4\", r\"(^|\\s)(abd)($|\\s|\\.)\": r\"\\1amerika birleşik devletleri\\3\", r\"(^|\\s)(ab)($|\\s|\\.)\": r\"\\1avrupa birliği\\3\", r\"(^|\\s)(bm)($|\\s|\\.)\": r\"\\1birleşmiş milletler\\3\", r\"(^|\\s)(a(i|ı)hm)($|\\s|\\.)\": r\"\\1avrupa insan hakları mahkemesi\\4\", r\"(^|\\s)(a(i|ı)hs)($|\\s|\\.)\": r\"\\1avrupa insan hakları sözleşmesi\\4\", r\"(^|\\s)(möhuk)($|\\s|\\.)\": r\"\\1milletlerarası özel hukuk ve usul hukuku\\3\", r\"(^|\\s)(fsek)($|\\s|\\.)\": r\"\\1fikir ve sanat eserleri kanunu\\3\", r\"(^|\\s)(kktc)($|\\s|\\.)\": r\"\\1kuzey kıbrıs türk cumhuriyeti\\3\", r\"(^|\\s)([ey]?tkhk)($|\\s|\\.)\": r\"\\1tüketicinin korunması hakkında kanun\\3\", r\"(^|\\s)(khk)($|\\s|\\.)\": r\"\\1kanun hükmünde kararname\\3\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_file = open('./stopwords2.txt')\n",
    "stop_words = sw_file.read()\n",
    "stop_words = stop_words.split(\"\\n\")\n",
    "sw_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(string, kw=False):\n",
    "    for key in STRING_CONVERSION.keys():\n",
    "        string = string.replace(key, STRING_CONVERSION[key])\n",
    "    string = string.lower()\n",
    "    result = []\n",
    "    for char in string:\n",
    "        if unicodedata.combining(char):\n",
    "            try:\n",
    "                result[-1] = chr(CHAR_CONVERSION[ord(char)][ord(result[-1])])\n",
    "            except:\n",
    "                continue\n",
    "        elif ord(char) in CHAR_CONVERSION.keys():\n",
    "            if CHAR_CONVERSION[ord(char)] != -1:\n",
    "                result.append(chr(CHAR_CONVERSION[ord(char)]))\n",
    "        elif ord(char) >= 942:\n",
    "            continue\n",
    "        else:\n",
    "            result.append(char)\n",
    "    result_str = ''.join(result)\n",
    "    result_str = ' '.join([w for w in result_str.split(\" \") if w != \"\"])\n",
    "    for key in ABBREVIATIONS.keys():\n",
    "        result_str = re.sub(key, ABBREVIATIONS[key], result_str)\n",
    "    if kw:\n",
    "        for key in REGEX_CONVERSION.keys():\n",
    "            result_str = re.sub(key, REGEX_CONVERSION[key], result_str)\n",
    "    return((result_str).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_list(li, kw=False):\n",
    "    clean_list = []\n",
    "    for item in li:\n",
    "        if len(item.split(\",\")) > 1:\n",
    "            clean_list.extend([it for it in item.split(\",\") if it != \"\"])\n",
    "        elif len(item.split(\";\")) > 1:\n",
    "            clean_list.extend([it for it in item.split(\";\") if it != \"\"])\n",
    "        else:\n",
    "            clean_list.append(item)\n",
    "    clean_list = [clean_text(text, kw) for text in clean_list if text != \"\"]\n",
    "    if not kw:\n",
    "        clean_str = ' '.join(clean_list)\n",
    "        for key in REGEX_CONVERSION.keys():\n",
    "            clean_str = re.sub(key, REGEX_CONVERSION[key], clean_str)\n",
    "        clean_list = [part for part in clean_str.split(\" \") if part != \"\"]\n",
    "        clean_list = [text for text in clean_list if text not in stop_words]\n",
    "    if kw:\n",
    "        clean_list = list(dict.fromkeys(clean_list))\n",
    "    clean_list = [text for text in clean_list if text != \"\"]\n",
    "    return clean_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_obj in json_data:\n",
    "    data_obj[\"Metin\"] = re.sub(r\"[-\\w\\.]+@([-\\w]+\\.)+[-\\w]{2,4}\", \"\", data_obj[\"Metin\"])\n",
    "    data_obj[\"Metin\"] = re.sub(r\"https?:\\/\\/(?:www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b(?:[-a-zA-Z0-9()@:%_\\+.~#?&\\/=]*)\", \"\", data_obj[\"Metin\"])\n",
    "    data_obj[\"Metin\"] = re.sub(r\"<([a-z]+)(?![^>]*\\/>)[^>]*>\", \"\", data_obj[\"Metin\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [(data_obj[\"FileName\"], clean_list(data_obj[\"Metin\"].split(\" \"))) for data_obj in json_data]\n",
    "words = [(wtup[0], len(wtup[1]), wtup[1]) for wtup in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [(data_obj[\"FileName\"], clean_list(data_obj[\"Anahtar Kelimeler\"], True)) for data_obj in json_data]\n",
    "keywords = [(kwtup[0], len(kwtup[1]), kwtup[1]) for kwtup in keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [wtup[2] for wtup in words]\n",
    "y = [kwtup[2] for kwtup in keywords]\n",
    "data = [[' '.join(words[i][2]), ','.join(keywords[i][2])] for i,_ in enumerate(words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anonim ortaklıkların amacı kime hizmet ettiği ...</td>\n",
       "      <td>menfaat sahipliği teorisi,insan sermayesi,takı...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uluslararası kamu hukuku devletlerin kimi zama...</td>\n",
       "      <td>ısrarlı itirazcı,itiraz,teamül hukuku,uluslara...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vesayet altındaki kişinin malvarlığının yöneti...</td>\n",
       "      <td>vesayet,vasi,vesayet makamı,vesayet altındaki ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dolandırıcılık doğru davranma iyiniyet kuralla...</td>\n",
       "      <td>türk ceza kanunu,suç,ceza,hile,dolandırıcılık,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>türkiye 2019 yılının ilk altı ayında yaklaşık ...</td>\n",
       "      <td>gümrük,döviz,mülkiyet hakkı,idari para cezası,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  anonim ortaklıkların amacı kime hizmet ettiği ...   \n",
       "1  uluslararası kamu hukuku devletlerin kimi zama...   \n",
       "2  vesayet altındaki kişinin malvarlığının yöneti...   \n",
       "3  dolandırıcılık doğru davranma iyiniyet kuralla...   \n",
       "4  türkiye 2019 yılının ilk altı ayında yaklaşık ...   \n",
       "\n",
       "                                                tags  \n",
       "0  menfaat sahipliği teorisi,insan sermayesi,takı...  \n",
       "1  ısrarlı itirazcı,itiraz,teamül hukuku,uluslara...  \n",
       "2  vesayet,vasi,vesayet makamı,vesayet altındaki ...  \n",
       "3  türk ceza kanunu,suç,ceza,hile,dolandırıcılık,...  \n",
       "4  gümrük,döviz,mülkiyet hakkı,idari para cezası,...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data, columns = ['text', 'tags'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_binary = CountVectorizer(tokenizer = lambda x: x.split(\",\"), binary=True)\n",
    "y_multilabel = vectorizer_binary.fit_transform(df['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_sum = y_multilabel.sum(axis=0).tolist()[0]\n",
    "sorted_tags_i = sorted(range(len(tags_sum)), key=lambda i: tags_sum[i], reverse=True)\n",
    "yn_multilabel = y_multilabel[:, sorted_tags_i[:1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(655, 4)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tag_count_1000'] = [sum(yn_multilabel[i].toarray()[0]) for i in range(yn_multilabel.shape[0])]\n",
    "df1 = df[df['tag_count_1000'] != 0]\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=0.00009, max_features=200000, tokenizer = lambda x: x.split(), ngram_range=(1, 3))\n",
    "clf = OneVsRestClassifier(SGDClassifier(loss='log_loss', alpha=0.0001, penalty='l2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of each fold - [0.10416666666666667, 0.10416666666666667, 0.06993006993006994, 0.0979020979020979, 0.07692307692307693]\n",
      "Average accuracy : 0.09061771561771562\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "scores = np.zeros((k, 5))\n",
    "k_idx = 0\n",
    "for train_index, test_index in kf.split(df, yn_multilabel):\n",
    "    X_train, X_test = df.iloc[train_index, :], df.iloc[test_index, :]\n",
    "    y_train, y_test = yn_multilabel[train_index], yn_multilabel[test_index]\n",
    "    \n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train['text'])\n",
    "    X_test_tfidf = vectorizer.transform(X_test['text'])\n",
    "    print(\"Training:\", X_train_tfidf.shape, y_train.shape, \"Test:\", X_test_tfidf.shape, y_test.shape)\n",
    "    clf.fit(X_train_tfidf, y_train)\n",
    "    y_pred = clf.predict(X_test_tfidf)\n",
    "     \n",
    "    acc = metrics.accuracy_score(y_pred, y_test)\n",
    "    prec = metrics.precision_score(y_test, y_pred, average = 'micro')\n",
    "    recl = metrics.recall_score(y_test, y_pred, average = 'micro')\n",
    "    mif1 = metrics.f1_score(y_test, y_pred, average = 'micro')\n",
    "    maf1 = metrics.f1_score(y_test, y_pred, average = 'macro')\n",
    "    scores[k_idx,:] = np.array([acc, prec, recl, mif1, maf1])\n",
    "    k_idx += 1\n",
    "avg_scores = scores.mean(axis=0)\n",
    "print('Average accuracy : {}'.format(avg_scores[0]))\n",
    "print('Average precision : {}'.format(avg_scores[0]))\n",
    "print('Average recall : {}'.format(avg_scores[0]))\n",
    "print('Average accuracy : {}'.format(avg_scores[0]))\n",
    "print('Average accuracy : {}'.format(avg_scores[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points in training data : 573\n",
      "Number of data points in test data : 144\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df, yn_multilabel, test_size=0.2, random_state=42)\n",
    "print(\"Number of data points in training data :\", X_train.shape[0])\n",
    "print(\"Number of data points in test data :\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=0.00009, max_features=200000, tokenizer = lambda x: x.split(), ngram_range=(1, 3))\n",
    "X_train_multilabel = vectorizer.fit_transform(X_train['text'])\n",
    "X_test_multilabel = vectorizer.transform(X_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape X :  (573, 200000) Y : (573, 1000)\n",
      "Test data shape X :  (144, 200000) Y: (144, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data shape X : \", X_train_multilabel.shape, \"Y :\", y_train.shape)\n",
    "print(\"Test data shape X : \", X_test_multilabel.shape,\"Y:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = OneVsRestClassifier(SGDClassifier(loss='log_loss', alpha=0.0001, penalty='l1'))\n",
    "clf.fit(X_train_multilabel, y_train)\n",
    "y_pred = clf.predict(X_test_multilabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[128], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m clf \u001b[38;5;241m=\u001b[39m OneVsOneClassifier(SGDClassifier(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m, penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml1\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_multilabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_test_multilabel)\n",
      "File \u001b[0;32m~/Documents/Projects/cse7052-nlp/venv/lib/python3.9/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/cse7052-nlp/venv/lib/python3.9/site-packages/sklearn/multiclass.py:690\u001b[0m, in \u001b[0;36mOneVsOneClassifier.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit underlying estimators.\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \n\u001b[1;32m    676\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;124;03m    The fitted underlying estimator.\u001b[39;00m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;66;03m# We need to validate the data because we do a safe_indexing later.\u001b[39;00m\n\u001b[0;32m--> 690\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    692\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m check_classification_targets(y)\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
      "File \u001b[0;32m~/Documents/Projects/cse7052-nlp/venv/lib/python3.9/site-packages/sklearn/base.py:622\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    620\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 622\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    623\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/Documents/Projects/cse7052-nlp/venv/lib/python3.9/site-packages/sklearn/utils/validation.py:1162\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1143\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1144\u001b[0m     )\n\u001b[1;32m   1146\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m   1147\u001b[0m     X,\n\u001b[1;32m   1148\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1159\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1160\u001b[0m )\n\u001b[0;32m-> 1162\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43m_check_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_numeric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m check_consistent_length(X, y)\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m~/Documents/Projects/cse7052-nlp/venv/lib/python3.9/site-packages/sklearn/utils/validation.py:1183\u001b[0m, in \u001b[0;36m_check_y\u001b[0;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1182\u001b[0m     estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m-> 1183\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcolumn_or_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1184\u001b[0m     _assert_all_finite(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, estimator_name\u001b[38;5;241m=\u001b[39mestimator_name)\n\u001b[1;32m   1185\u001b[0m     _ensure_no_complex_data(y)\n",
      "File \u001b[0;32m~/Documents/Projects/cse7052-nlp/venv/lib/python3.9/site-packages/sklearn/utils/validation.py:1219\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, dtype, warn)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Ravel column or 1d numpy array, else raises an error.\u001b[39;00m\n\u001b[1;32m   1194\u001b[0m \n\u001b[1;32m   1195\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;124;03m    If `y` is not a 1D array or a 2D array with a single row or column.\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y)\n\u001b[0;32m-> 1219\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1228\u001b[0m shape \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m   1229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/Projects/cse7052-nlp/venv/lib/python3.9/site-packages/sklearn/utils/validation.py:881\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sp\u001b[38;5;241m.\u001b[39missparse(array):\n\u001b[1;32m    880\u001b[0m     _ensure_no_complex_data(array)\n\u001b[0;32m--> 881\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43m_ensure_sparse_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;66;03m# If np.array(..) gives ComplexWarning, then we convert the warning\u001b[39;00m\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;66;03m# to an error. This is needed because specifying a non complex\u001b[39;00m\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;66;03m# dtype to the function converts complex to real dtype,\u001b[39;00m\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;66;03m# thereby passing the test made in the lines following the scope\u001b[39;00m\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;66;03m# of warnings context manager.\u001b[39;00m\n\u001b[1;32m    897\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n",
      "File \u001b[0;32m~/Documents/Projects/cse7052-nlp/venv/lib/python3.9/site-packages/sklearn/utils/validation.py:532\u001b[0m, in \u001b[0;36m_ensure_sparse_format\u001b[0;34m(spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    529\u001b[0m _check_large_sparse(spmatrix, accept_large_sparse)\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accept_sparse \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m--> 532\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA sparse matrix was passed, but dense \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata is required. Use X.toarray() to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    535\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvert to a dense numpy array.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    536\u001b[0m     )\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(accept_sparse, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(accept_sparse) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array."
     ]
    }
   ],
   "source": [
    "clf = OneVsOneClassifier(SGDClassifier(loss='log_loss', alpha=0.0001, penalty='l1'))\n",
    "clf.fit(X_train_multilabel, y_train)\n",
    "y_pred = clf.predict(X_test_multilabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "VEC_SIZE = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = gensim.models.Word2Vec(X, min_count = 1, vector_size = VEC_SIZE, window = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_pooling(ds):\n",
    "    pooled = np.zeros((ds.shape[0], VEC_SIZE))\n",
    "    for i, row in enumerate(ds):\n",
    "        for word in row.split():\n",
    "            pooled[i] += word2vec_model.wv[word]\n",
    "        pooled[i] /= len(row)\n",
    "    return pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_wv = avg_pooling(X_train['text'])\n",
    "X_test_wv = avg_pooling(X_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape X :  (573, 500) Y : (573, 1000)\n",
      "Test data shape X :  (144, 500) Y: (144, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data shape X : \", X_train_wv.shape, \"Y :\", y_train.shape)\n",
    "print(\"Test data shape X : \", X_test_wv.shape,\"Y:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = LogisticRegression(multi_class=\"multinomial\")\n",
    "model.fit(X_train_wv, y_train)\n",
    "y_pred = clf.predict(X_test_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = OneVsRestClassifier(LogisticRegression(multi_class='multinomial', solver='lbfgs'))\n",
    "model = OneVsRestClassifier(SGDClassifier(loss='log_loss', alpha=0.0001))\n",
    "model.fit(X_train_wv, y_train)\n",
    "y_pred = model.predict(X_test_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.10416666666666667\n",
      "Micro Precision score : 0.34545454545454546\n",
      "Macro Precision score : 0.016\n",
      "Micro F1 score : 0.095\n",
      "Macro F1 score : 0.015285714285714284\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy :\", metrics.accuracy_score(y_test,y_pred))\n",
    "print(\"Micro Precision score :\", metrics.precision_score(y_test, y_pred, average = 'micro'))\n",
    "print(\"Macro Precision score :\", metrics.precision_score(y_test, y_pred, average = 'macro'))\n",
    "print(\"Micro F1 score :\", metrics.f1_score(y_test, y_pred, average = 'micro'))\n",
    "print(\"Macro F1 score :\", metrics.f1_score(y_test, y_pred, average = 'macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144, 1000) (144, 1000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 1000)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_test.shape, y_pred.shape)\n",
    "y_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array(['2017 anayasa değişiklikleri', 'alt işveren'], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array(['anonim ortaklık', 'bağışlayan', 'hakimin karar vermesi'],\n",
       "       dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array(['6271 sayılı kanun'], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array(['1992 hukuki sorumluluk sözleşmesi', 'af',\n",
       "        'ailenin korunması hakkı'], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array(['belge ibrazı'], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array(['akde aykırılık'], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array(['1946 seçimi'], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array(['1876'], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array(['12 aylık hoşgörü süresi'], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array(['2005 tarihli uluslararası sağlık tüzüğü ust', 'akdi sorumluluk',\n",
       "        'anayasa mahkemesi kararları', 'belirlilik', 'ciro edilemez kaydı',\n",
       "        'danıştay', 'delil', 'efor zararı', 'gemi kira sözleşmesi'],\n",
       "       dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array(['6098 sayılı türk borçlar kanunu madde 310', 'anayasa yargısı',\n",
       "        'askeri suç',\n",
       "        'avrupa insan hakları mahkemesi mesutoğlu türkiye kararı',\n",
       "        'biyoteknolojik buluş', 'deniz haydutluğu',\n",
       "        'düzenleme yetkisi kamu gücü ilişkisi', 'hesap verebilirlik'],\n",
       "       dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array(['abonelik sözleşmesi'], dtype='<U195'),\n",
       " array(['amerika birleşik devletleri ve suç'], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array(['12 aylık hoşgörü süresi', 'beyaz yaka suçları'], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array(['1951 sözleşmesi madde 1 f'], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array(['1879 tarihli ceza muhakemesi kanunu'], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array(['1946 seçimi'], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array(['bilişim sistemi'], dtype='<U195'),\n",
       " array(['1808 tarihli fransız ceza muhakemesi kanunu'], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array(['6098 sayılı türk borçlar kanunu madde 310', 'anayasa yargısı',\n",
       "        'askeri suç',\n",
       "        'avrupa insan hakları mahkemesi mesutoğlu türkiye kararı',\n",
       "        'biyoteknolojik buluş', 'deniz haydutluğu',\n",
       "        'düzenleme yetkisi kamu gücü ilişkisi', 'hesap verebilirlik'],\n",
       "       dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array(['antik dönemde tabii hukuk'], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array(['2017 anayasa değişiklikleri', 'alt işveren'], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array(['bilgi ve iletişim teknolojileri',\n",
       "        'borçlanma araçları sahipleri kurulu bask'], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array(['amerika birleşik devletleri anayasası 1 ek madde', 'bedel'],\n",
       "       dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195'),\n",
       " array([], dtype='<U195')]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_binary.inverse_transform(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
