{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import unicodedata\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = []\n",
    "for path, dirs, files in os.walk(f\"./TrainingSet\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".json\"):\n",
    "            f = open(os.path.join(path, file), encoding=\"utf-8-sig\")\n",
    "            json_data.append(json.load(f))\n",
    "            json_data[-1][\"FileName\"] = file\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAR_CONVERSION = {9:32,10:32,33:32,35:32,38:32,40:32,41:32,42:32,43:32,47:45,58:32,59:32,60:-1,61:32,62:-1,63:32,64:32,91:-1,93:-1,94:39,95:45,96:39,123:-1,124:32,125:-1,160:32,167:32,170:-1,171:32,176:-1,180:39,182:-1,184:-1,187:32,223:-1,224:97,225:97,226:97,228:97,229:97,230:101,232:101,233:101,234:101,235:101,236:105,237:105,238:105,239:105,241:110,242:111,243:111,244:111,245:111,248:111,249:117,250:117,251:117,257:97,261:97,263:99,269:99,279:101,299:105,322:108,324:110,353:115,363:117,369:252,382:122,523:105,537:351,601:101,699:39,700:39,703:39,706:-1,714:39,727:-1,774:{103: 287},775:{105: 105},8201:32,8208:45,8211:45,8212:45,8216:39,8217:39,8220:-1,8221:-1,8232:32, }\n",
    "STRING_CONVERSION = {\"$\": \"dolar\",\"…\": \" \",\"ﬀ\": \"ff\",\"ﬁ\": \"fi\",\"ﬂ \": \"fl\",\"ﬂ\": \"fl\",\"ﬄ\": \"ffl\",\"n°\": \"no\",\"N°\": \"no\",\"Anahtar Kelimeler:\": \"\",\"I\": \"ı\",\"İ\": \"i\",\"⅔\": \"%67\",\"⅕\": \"%20\",\"⅘\": \"%80\",\"⅚\": \"%83\",\"¼\": \"%25\",\"½\": \"%50\",\"¾\": \"%75\",}\n",
    "REGEX_CONVERSION = {r\"cov[ı,i]d-*\\s*19\": \"covid19\",r\"(\\smd?\\.?\\s{0,2})((\\d)+)\": r\" madde \\2\",r\"<([a-z]+)(?![^>]*\\/>)[^>]*>\": \"\",r\"\\.\": \" \",r\"-\": \" \",r\"'\": \" \",r\"(\\s(%)(\\s){0,2})((\\d)*)\": r\" yüzde \\4\", r\"%\": \"\", r\"hükumet\": \"hükümet\", r\"ışletme\": \"işletme\", r\"prepayment\": \"önödeme\", r\"arabulucui\": \"arabulucu\"}\n",
    "ABBREVIATIONS = {r\"(^|\\s)(smk)($|\\s|\\.)\":r\"\\1sınai mülkiyet kanunu\\3\", r\"(^|\\s)(t?mk)($|\\s|\\.)\": r\"\\1türk medeni kanunu\\3\", r\"(^|\\s)(t?ck)($|\\s|\\.)\": r\"\\1türk ceza kanunu\\3\", r\"(^|\\s)(t?bk)($|\\s|\\.)\": r\"\\1türk borçlar kanunu\\3\", r\"(^|\\s)(t?tk)($|\\s|\\.)\": r\"\\1türk ticaret kanunu\\3\", r\"(^|\\s)(t?vk)($|\\s|\\.)\": r\"\\1türk vergi kanunu\\3\", r\"(^|\\s)(a(b|t)ad)($|\\s|\\.)\": r\"\\1avrupa birliği adalet divanı\\4\", r\"(^|\\s)(abd)($|\\s|\\.)\": r\"\\1amerika birleşik devletleri\\3\", r\"(^|\\s)(ab)($|\\s|\\.)\": r\"\\1avrupa birliği\\3\", r\"(^|\\s)(bm)($|\\s|\\.)\": r\"\\1birleşmiş milletler\\3\", r\"(^|\\s)(a(i|ı)hm)($|\\s|\\.)\": r\"\\1avrupa insan hakları mahkemesi\\4\", r\"(^|\\s)(a(i|ı)hs)($|\\s|\\.)\": r\"\\1avrupa insan hakları sözleşmesi\\4\", r\"(^|\\s)(möhuk)($|\\s|\\.)\": r\"\\1milletlerarası özel hukuk ve usul hukuku\\3\", r\"(^|\\s)(fsek)($|\\s|\\.)\": r\"\\1fikir ve sanat eserleri kanunu\\3\", r\"(^|\\s)(kktc)($|\\s|\\.)\": r\"\\1kuzey kıbrıs türk cumhuriyeti\\3\", r\"(^|\\s)([ey]?tkhk)($|\\s|\\.)\": r\"\\1tüketicinin korunması hakkında kanun\\3\", r\"(^|\\s)(khk)($|\\s|\\.)\": r\"\\1kanun hükmünde kararname\\3\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_file = open('./stopwords2.txt')\n",
    "stop_words = sw_file.read()\n",
    "stop_words = stop_words.split(\"\\n\")\n",
    "sw_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(string, kw=False):\n",
    "    for key in STRING_CONVERSION.keys():\n",
    "        string = string.replace(key, STRING_CONVERSION[key])\n",
    "    string = string.lower()\n",
    "    result = []\n",
    "    for char in string:\n",
    "        if unicodedata.combining(char):\n",
    "            try:\n",
    "                result[-1] = chr(CHAR_CONVERSION[ord(char)][ord(result[-1])])\n",
    "            except:\n",
    "                continue\n",
    "        elif ord(char) in CHAR_CONVERSION.keys():\n",
    "            if CHAR_CONVERSION[ord(char)] != -1:\n",
    "                result.append(chr(CHAR_CONVERSION[ord(char)]))\n",
    "        elif ord(char) >= 942:      # greek and cyrillic alphabet\n",
    "            continue\n",
    "        else:\n",
    "            result.append(char)\n",
    "    result_str = ''.join(result)\n",
    "    result_str = ' '.join([w for w in result_str.split(\" \") if w != \"\"])\n",
    "    for key in ABBREVIATIONS.keys():\n",
    "        result_str = re.sub(key, ABBREVIATIONS[key], result_str)\n",
    "    if kw:\n",
    "        for key in REGEX_CONVERSION.keys():\n",
    "            result_str = re.sub(key, REGEX_CONVERSION[key], result_str)\n",
    "    return((result_str).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_list(li, kw=False):\n",
    "    clean_list = []\n",
    "    for item in li:\n",
    "        if len(item.split(\",\")) > 1:\n",
    "            clean_list.extend([it for it in item.split(\",\") if it != \"\"])\n",
    "        elif len(item.split(\";\")) > 1:\n",
    "            clean_list.extend([it for it in item.split(\";\") if it != \"\"])\n",
    "        else:\n",
    "            clean_list.append(item)\n",
    "    clean_list = [clean_text(text, kw) for text in clean_list if text != \"\"]\n",
    "    if not kw:\n",
    "        clean_str = ' '.join(clean_list)\n",
    "        for key in REGEX_CONVERSION.keys():\n",
    "            clean_str = re.sub(key, REGEX_CONVERSION[key], clean_str)\n",
    "        clean_list = [part for part in clean_str.split(\" \") if part != \"\"]\n",
    "        # clean_list = [stemmer.stemWord(text) for text in clean_list if text not in stop_words]\n",
    "        clean_list = [text for text in clean_list if text not in stop_words]\n",
    "    if kw:\n",
    "        clean_list = list(dict.fromkeys(clean_list))\n",
    "    # clean_list = [' '.join([part for part in text.split(\" \") if part not in stop_words and part != \"\"]) for text in clean_list]\n",
    "    clean_list = [text for text in clean_list if text != \"\"]\n",
    "    return clean_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_obj in json_data:\n",
    "    data_obj[\"Metin\"] = re.sub(r\"[-\\w\\.]+@([-\\w]+\\.)+[-\\w]{2,4}\", \"\", data_obj[\"Metin\"])\n",
    "    data_obj[\"Metin\"] = re.sub(r\"https?:\\/\\/(?:www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b(?:[-a-zA-Z0-9()@:%_\\+.~#?&\\/=]*)\", \"\", data_obj[\"Metin\"])\n",
    "    data_obj[\"Metin\"] = re.sub(r\"<([a-z]+)(?![^>]*\\/>)[^>]*>\", \"\", data_obj[\"Metin\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [(data_obj[\"FileName\"], clean_list(data_obj[\"Metin\"].split(\" \"))) for data_obj in json_data]\n",
    "words = [(wtup[0], len(wtup[1]), wtup[1]) for wtup in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [(data_obj[\"FileName\"], clean_list(data_obj[\"Anahtar Kelimeler\"], True)) for data_obj in json_data]\n",
    "keywords = [(kwtup[0], len(kwtup[1]), kwtup[1]) for kwtup in keywords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w_flat = [(w, wtup[0]) for wtup in words for w in wtup[2]]\n",
    "kw_flat = [(kw, kwtup[0]) for kwtup in keywords for kw in kwtup[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def unique_chars_and_words(list_flat):\n",
    "    list_unique = []\n",
    "    list_dict = {}\n",
    "    list_chars = []\n",
    "    list_charcodes = []\n",
    "    for w in list_flat:\n",
    "        if w[0] not in list_unique:\n",
    "            list_unique.append(w[0])\n",
    "            list_dict[w[0]] = [w[1]]\n",
    "        else:\n",
    "            list_dict[w[0]].append(w[1])\n",
    "        for c in w[0]:\n",
    "            if c not in list_chars:\n",
    "                list_chars.append(c)\n",
    "                list_charcodes.append((ord(c), w[0]))\n",
    "    list_chars = [[list_chars[i], list_charcodes[i][0], list_charcodes[i][1]] for i, c in enumerate(list_chars)]\n",
    "    list_unique = [[list_unique[i], len(list_dict[list_unique[i]]), list_dict[list_unique[i]]] for i,_ in enumerate(list_unique)]\n",
    "    return (list_chars, list_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kw_chars, kw_unique = unique_chars_and_words(kw_flat)\n",
    "w_chars, w_unique = unique_chars_and_words(w_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [wtup[2] for wtup in words]\n",
    "y = [kwtup[2] for kwtup in keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "allkws = [w for row in y for kw in row for w in kw.split(\" \")]\n",
    "allws = [w for row in X for w in row]\n",
    "nomatchkws = [kw for kw in allkws if kw not in allws]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nazimaniltepe/Documents/Projects/cse7052-nlp/venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = gensim.models.Word2Vec(X, min_count = 1, vector_size = DIM_SIZE, window = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242458"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2vec_model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hukuku', 1.0000001192092896),\n",
       " ('hukukunun', 0.7701903581619263),\n",
       " ('hukukuna', 0.7288190126419067),\n",
       " ('geçirilmiş', 0.632036566734314),\n",
       " ('hukukundaki', 0.6297780275344849),\n",
       " ('hukukuyla', 0.6232560873031616),\n",
       " ('hukukunda', 0.6179437637329102),\n",
       " ('hukukunu', 0.6174837946891785),\n",
       " ('vazedilmeye', 0.616786777973175),\n",
       " ('hukukundan', 0.6048429608345032)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asd = word2vec_model.wv[\"hukuku\"]\n",
    "word2vec_model.wv.most_similar(positive=[asd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_np = np.zeros((len(X), DIM_SIZE))\n",
    "for i, row in enumerate(X):\n",
    "    for word in row:\n",
    "        X_np[i] += word2vec_model.wv[word]\n",
    "    X_np[i] /= len(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec_model.build_vocab([[\"noword\"]], update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mlb = MultiLabelBinarizer()\n",
    "y_binary = y_mlb.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mlb = MultiLabelBinarizer()\n",
    "X_binary = X_mlb.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_np, y_binary, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_train = min_max_scaler.fit_transform(X_train)\n",
    "X_test = min_max_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nazimaniltepe/Documents/Projects/cse7052-nlp/venv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, hidden_layer_sizes=15,\n",
       "              learning_rate=&#x27;adaptive&#x27;, max_iter=2000, random_state=1,\n",
       "              solver=&#x27;lbfgs&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, hidden_layer_sizes=15,\n",
       "              learning_rate=&#x27;adaptive&#x27;, max_iter=2000, random_state=1,\n",
       "              solver=&#x27;lbfgs&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(activation='logistic', hidden_layer_sizes=15,\n",
       "              learning_rate='adaptive', max_iter=2000, random_state=1,\n",
       "              solver='lbfgs')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(solver='lbfgs', activation='logistic', max_iter=2000, learning_rate='adaptive', hidden_layer_sizes=(15), random_state=1)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in y_pred:\n",
    "    for col in row:\n",
    "        if col > 0:\n",
    "            print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[()]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_mlb.inverse_transform(y_pred[0:1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = y_pred[0:1,:].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
